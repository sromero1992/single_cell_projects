{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.sparse import issparse\n",
    "import scanpy as sc\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.sparse import csr_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binning_method(data, method=\"rice\"):\n",
    "    \"\"\"\n",
    "    Calculates the optimal binning for the given data based on the selected method.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data for which to calculate the bins.\n",
    "        method (str): The binning method. Options:\n",
    "                      \"auto\", \"square_root\", \"rice\", \"logarithmic\", \"freedman-diaconis\", \"scott\".\n",
    "    \n",
    "    Returns:\n",
    "        bins (ndarray or int): The bin edges (for \"auto\" method) or the number of bins (for other methods).\n",
    "    \"\"\"\n",
    "    # Ensure data is a numpy array\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Get the number of data points\n",
    "    n = len(data)\n",
    "    \n",
    "    # If method is 'rice'\n",
    "    if method == \"rice\":\n",
    "        # Rice Rule: Number of bins = 2 * n^(1/3)\n",
    "        return int(np.ceil(2 * n**(1/3)))\n",
    "\n",
    "    # If method is 'logarithmic'\n",
    "    elif method == \"logarithmic\":\n",
    "        # Logarithmic Binning: Number of bins = log2(n) + 1\n",
    "        return int(np.ceil(np.log2(n) + 1))\n",
    "    \n",
    "    # If method is 'square_root'\n",
    "    elif method == \"square_root\":\n",
    "        # Square Root Rule: Number of bins = sqrt(n)\n",
    "        return int(np.ceil(np.sqrt(n)))\n",
    "    \n",
    "    # If method is 'freedman-diaconis'\n",
    "    elif method == \"freedman-diaconis\":\n",
    "        # Freedman-Diaconis Rule: Bin width = (2 * IQR) / n^(1/3)\n",
    "        q1, q3 = np.percentile(data, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        bin_width = 2 * iqr / (n ** (1/3))\n",
    "        return int(np.ceil((np.max(data) - np.min(data)) / bin_width))\n",
    "    \n",
    "    # If method is 'scott'\n",
    "    elif method == \"scott\":\n",
    "        # Scott's Rule: Bin width = (3.5 * std(data)) / (n^(1/3))\n",
    "        bin_width_scott = (3.5 * np.std(data)) / (n ** (1/3))\n",
    "        return int(np.ceil((np.max(data) - np.min(data)) / bin_width_scott))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from: 'auto', 'square_root', 'rice', 'logarithmic', 'freedman-diaconis', 'scott'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_matrix(matrix, nbins=20, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Computes the mutual information matrix in parallel, working directly with sparse matrices,\n",
    "    and computes the full matrix (including the diagonal elements).\n",
    "    \"\"\"\n",
    "    if not issparse(matrix):\n",
    "        matrix = csr_matrix(matrix)\n",
    "\n",
    "    n_features = matrix.shape[0]\n",
    "    mi_matrix = np.zeros((n_features, n_features))\n",
    "\n",
    "    def compute_pairwise_mi(i, j, matrix, nbins=20):\n",
    "        \"\"\"\n",
    "        Computes mutual information between row i and row j of the sparse matrix.\n",
    "        \"\"\"\n",
    "        vi = matrix[i, :].toarray().flatten()\n",
    "        vj = matrix[j, :].toarray().flatten()\n",
    "\n",
    "        # Select binning method\n",
    "        x_bins = binning_method(vi)\n",
    "        y_bins = binning_method(vj)\n",
    "        # Print the bins\n",
    "        print(f\"Bins for feature {i} (x):\", x_bins)\n",
    "        print(f\"Bins for feature {j} (y):\", y_bins)\n",
    "                \n",
    "        joint_counts, _, _ = np.histogram2d(vi, vj, bins=[x_bins, y_bins])\n",
    "        ncounts = joint_counts.sum()\n",
    "        if ncounts == 0:\n",
    "            return 0  # No mutual information if no overlap\n",
    "        joint_prob = joint_counts / ncounts + 1e-10\n",
    "\n",
    "        marginal_i = joint_prob.sum(axis=1) + 1e-10\n",
    "        marginal_j = joint_prob.sum(axis=0) + 1e-10\n",
    "        \n",
    "        joint_prob = joint_prob.flatten()\n",
    "        h_xy =  -np.sum(joint_prob * np.log2(joint_prob))\n",
    "        h_x =  -np.sum(marginal_i * np.log2(marginal_i))\n",
    "        h_y =  -np.sum(marginal_j * np.log2(marginal_j))\n",
    "\n",
    "        return float(h_x + h_y - h_xy)\n",
    "\n",
    "    # Parallelizing the pairwise mutual information computation\n",
    "    jobs = [(i, j) for i in range(n_features) for j in range(i+1, n_features)]  # Includes diagonal\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_pairwise_mi)(i, j, matrix, nbins) for i, j in jobs\n",
    "    )\n",
    "\n",
    "    # Fill the matrix with the results\n",
    "    for idx, (i, j) in enumerate(jobs):\n",
    "        mi_matrix[i, j] = results[idx]\n",
    "        mi_matrix[j, i] = results[idx]  # Exploit symmetry to avoid duplicate computation\n",
    "\n",
    "    return mi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "\n",
    "def compute_pearson_residual(matrix, theta=100):\n",
    "    \"\"\"\n",
    "    Compute clipped Pearson residuals for a given sparse matrix, where rows are features and columns are cells.\n",
    "    Reference: https://doi.org/10.1101/2020.12.01.405886\n",
    "\n",
    "    Args:\n",
    "        matrix (scipy.sparse.csr_matrix or numpy.ndarray): 2D sparse or dense array with rows as features and columns as cells.\n",
    "    \n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Clipped Pearson residuals matrix in sparse format.\n",
    "    \"\"\"\n",
    "    # Ensure the input matrix is sparse\n",
    "    if not issparse(matrix):\n",
    "        matrix = csr_matrix(matrix)\n",
    "    \n",
    "    # Compute the row and column sums\n",
    "    row_sums = matrix.sum(axis=1)  # Sum along rows (features)\n",
    "    col_sums = matrix.sum(axis=0)  # Sum along columns (cells)\n",
    "    total_sum = matrix.sum()       # Total sum of all elements\n",
    "\n",
    "    # Compute the expected values under independence assumption\n",
    "    row_sums_dense = np.array(row_sums).flatten()  # Convert to dense\n",
    "    col_sums_dense = np.array(col_sums).flatten()  # Convert to dense\n",
    "    expected = (row_sums_dense[:, None] * col_sums_dense[None, :]) / total_sum\n",
    "\n",
    "    # Ensure expected values are sparse for consistency\n",
    "    expected_sparse = csr_matrix(expected)\n",
    "\n",
    "    # Standard deviation for Pearson residuals\n",
    "    std_dev = np.sqrt(expected_sparse + (expected_sparse.power(2)) / theta)\n",
    "\n",
    "    # Compute Pearson residuals\n",
    "    residuals = (matrix - expected_sparse).multiply(std_dev.power(-1))  # Element-wise division\n",
    "\n",
    "    # Replace NaN and Inf values (caused by division by zero in std_dev) with 0\n",
    "    residuals.data[np.isnan(residuals.data)] = 0\n",
    "    residuals.data[np.isinf(residuals.data)] = 0\n",
    "\n",
    "    # Clip residuals to ±sqrt(n), where n is the number of columns (cells)\n",
    "    num_cells = matrix.shape[1]\n",
    "    clip_value = np.sqrt(num_cells)\n",
    "    residuals.data = np.clip(residuals.data, -clip_value, clip_value)\n",
    "\n",
    "    return residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 4697 × 5000\n",
      "    obs: 'CellID', 'BatchID', 'ClusterID', 'CellType', 'CellCycle', 'cell_potency', 'monocle3_pseudotime', 'splinefit_pseudotime', 'Tmonocleout'\n",
      "    layers: 'counts'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "\n",
    "# Download h5ad here: https://drive.google.com/drive/folders/1-2og2FAM0_6e3L2_9C7HnOm9sgfrJIRe?usp=sharing\n",
    "# Load data\n",
    "base_path = r\"C:\\Users\\ssromerogon\\Documents\\vscode_working_dir\\QUBO_Feature_Selection\\qubo_fs_dwave\\efficient_differentiation\\5000_HVGs\"\n",
    "fname = r\"Data_hESC_EC_day1_5000g_filtered_feature_bc_matrix_h5.h5ad\"\n",
    "\n",
    "# Use os.path.join for cross-platform compatibility\n",
    "file_dir = os.path.join(base_path, fname)\n",
    "\n",
    "# Read the AnnData object\n",
    "adata = sc.read_h5ad(file_dir)\n",
    "\n",
    "adata.layers['counts'] = adata.X.copy() \n",
    "# Print the AnnData object summary\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4697)\n",
      "(5000,)\n",
      "(1, 4697)\n",
      "(5001, 4697)\n"
     ]
    }
   ],
   "source": [
    "# Apply pearson residuals\n",
    "#sc.experimental.pp.normalize_pearson_residuals(adata, theta=100)\n",
    "#X = np.array(adata.X.T)\n",
    "X = adata.X.T\n",
    "X = compute_pearson_residual(X)\n",
    "X = X.toarray()\n",
    "y = np.array(adata.obs['monocle3_pseudotime'].values)\n",
    "y_reshaped = y.reshape(1, -1) \n",
    "g = adata.var_names\n",
    "print(X.shape)\n",
    "print(g.shape)\n",
    "print(y_reshaped.shape)\n",
    "Xy = np.vstack((X, y_reshaped)) \n",
    "Xy = csr_matrix(Xy)\n",
    "X = csr_matrix(X)\n",
    "\n",
    "print(Xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for MI construction: 1249.0782 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "MI_mat = mutual_information_matrix(Xy, nbins=20, n_jobs=-1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time for MI construction: {elapsed_time:.4f} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import root_scalar, minimize\n",
    "import time\n",
    "from dwave.samplers import SteepestDescentSolver\n",
    "\n",
    "K = 100\n",
    "# Redundancy matrix and importance vector from MI_mat and K\n",
    "R = MI_mat[:-1, :-1] / (K - 1)\n",
    "J = MI_mat[-1, :-1]\n",
    "\n",
    "# Define the optimize_qubo function (using SteepestDescentSolver to solve QUBO)\n",
    "def optimize_qubo(alpha, R, J):\n",
    "    \"\"\"\n",
    "    Computes the QUBO matrix and solves it using the Steepest Descent Solver.\n",
    "    \n",
    "    Parameters:\n",
    "    - alpha (float): Mixing optimal parameter.\n",
    "    - R (numpy.ndarray): Redundancy matrix (MI of feature-feature).\n",
    "    - J (numpy.ndarray): Importance vector (MI of target-features).\n",
    "    \n",
    "    Returns:\n",
    "    - n (int): Number of selected features (non-zero in solution vector).\n",
    "    - result (dimod.SampleSet): QUBO solver results.\n",
    "    \"\"\"\n",
    "    # Compute Q matrix\n",
    "    Qmat = (1 - alpha) * R - alpha * np.diag(J)\n",
    "    \n",
    "    # Convert QUBO matrix to dictionary format\n",
    "    def qubo_matrix_to_dict(Q):\n",
    "        qubo = {}\n",
    "        for i in range(Q.shape[0]):\n",
    "            for j in range(Q.shape[1]):\n",
    "                if Q[i, j] != 0:\n",
    "                    qubo[(i, j)] = Q[i, j]\n",
    "        return qubo\n",
    "    \n",
    "    qubo = qubo_matrix_to_dict(Qmat)\n",
    "    \n",
    "    # Initialize the Steepest Descent Solver\n",
    "    sampler = SteepestDescentSolver()\n",
    "    \n",
    "    # Solve the QUBO problem\n",
    "    result = sampler.sample_qubo(qubo, num_reads=100)\n",
    "    \n",
    "    print(f\"Energy from the solution: {result.first.energy}\")  # Print energy of the solution\n",
    "\n",
    "    # Number of selected features (non-zero elements in the best solution vector)\n",
    "    n = sum(result.first.sample.values())\n",
    "\n",
    "    print(f\"n solution: {n}\")  # Print energy of the solution\n",
    "\n",
    "    return n, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alpha: 0.0\n",
      "Energy from the solution: 9.000359568744898e-09\n",
      "n solution: 1\n",
      "Testing alpha: 1.0\n",
      "Energy from the solution: -605.7357724436584\n",
      "n solution: 5000\n",
      "Testing alpha: 0.5\n",
      "Energy from the solution: -4.726462112615991\n",
      "n solution: 82\n",
      "Testing alpha: 0.75\n",
      "Energy from the solution: -14.293549381241974\n",
      "n solution: 214\n",
      "Testing alpha: 0.625\n",
      "Energy from the solution: -8.232391475532495\n",
      "n solution: 128\n",
      "Testing alpha: 0.5625\n",
      "Energy from the solution: -6.270376354626933\n",
      "n solution: 103\n",
      "Testing alpha: 0.53125\n",
      "Energy from the solution: -5.455345323567599\n",
      "n solution: 93\n",
      "Testing alpha: 0.546875\n",
      "Energy from the solution: -5.85104980406868\n",
      "n solution: 97\n",
      "Testing alpha: 0.5546875\n",
      "Energy from the solution: -6.057486188678013\n",
      "n solution: 101\n",
      "Testing alpha: 0.55078125\n",
      "Energy from the solution: -5.953716814117797\n",
      "n solution: 99\n",
      "Testing alpha: 0.552734375\n",
      "Energy from the solution: -6.0054158741531865\n",
      "n solution: 99\n",
      "Testing alpha: 0.5537109375\n",
      "Energy from the solution: -6.031409832396093\n",
      "n solution: 100\n",
      "Optimal alpha value from root_scalar: 0.5537109375\n",
      "Energy from the solution: -6.031409832396093\n",
      "n solution: 100\n",
      "Energy from the solution: -6.031409832396093\n",
      "Time taken: 607.83 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import root_scalar, minimize\n",
    "import time\n",
    "from dwave.samplers import SteepestDescentSolver\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Objective function to optimize\n",
    "def objective(alpha):\n",
    "    print(f\"Testing alpha: {alpha}\")  # Print tested alpha values\n",
    "    return optimize_qubo(alpha, R, J)[0] - K\n",
    "\n",
    "# Try to find the optimal alpha using root_scalar (similar to fzero)\n",
    "try:\n",
    "    result = root_scalar(objective, bracket=[0, 1], method='bisect')\n",
    "    alphasol = result.root\n",
    "    print(f\"Optimal alpha value from root_scalar: {alphasol}\")\n",
    "\n",
    "    _, xsol = optimize_qubo(alphasol, R, J)\n",
    "    print(f\"Energy from the solution: {xsol.first.energy}\")  # Print energy of the solution\n",
    "\n",
    "except:\n",
    "    # If root_scalar fails, fall back to minimize (similar to fminsearch)\n",
    "    result = minimize(objective, x0=1, method='Nelder-Mead')\n",
    "    alphasol = result.x[0]\n",
    "    print(f\"Optimal alpha value from minimize: {alphasol}\")\n",
    "\n",
    "    _, xsol = optimize_qubo(alphasol, R, J)\n",
    "    print(f\"Energy from the solution: {xsol.first.energy}\")  # Print energy of the solution\n",
    "\n",
    "# End timing\n",
    "time_zerof = time.time() - start_time\n",
    "\n",
    "# Print computation time\n",
    "print(f\"Time taken: {time_zerof:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
